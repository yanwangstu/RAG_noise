# evaluate the experiment result based on the correct answer
import json
import string
import re
from tqdm import tqdm
from typing import Tuple
from openai import OpenAI
from ratelimit import limits, sleep_and_retry


# constant for evaluator LLM
RPM_LIMIT = 45
MAX_NEW_TOKENS = 200
# 派欧云API 付费版
MODEL_NAME_DICT = {"model_name": "Deepseek-V3-ppin",
                   "base_url": "https://api.ppinfra.com/v3/openai", 
                   "model_id": "deepseek/deepseek-v3-turbo", 
                   "api_key": "sk_2rwMr_cIXz02Ss7vi3yELfOP1J2D1DBPotAh6sDkvq8"}


# calculate the answer reject rate
# and identify the answer reject of each sample
def answer_reject_calculate(llm_answers: dict) -> Tuple[float, dict]:
    
    total_samples = len(llm_answers)
    answer_reject_samples = 0
    answer_reject_dict = {}
    
    for QID, llm_answer in llm_answers.items():
        if "I cannot answer" in llm_answer:
            answer_reject_samples += 1
            answer_reject_dict[QID] = True
        else:
            answer_reject_dict[QID] = False
    
    answer_reject_rate = answer_reject_samples/total_samples
    return answer_reject_rate, answer_reject_dict


# answer exact match
# if matched, return true, else return false
def answer_exact_match(llm_answer: str, correct_answer: str) -> bool:
    """
    Compare the values corresponding to QID specified in two dictionaries, 
    ignoring punctuation marks, spaces, and not case sensitive.

    : param llm_answers:  Dictionary containing answers generated by the model
    : param correct_answers:  A dictionary containing the correct answer
    : param QID:  Problem ID to be compared
    : return:  If the answer matches, return True; otherwise, return False
    """

    # Create a conversion table to remove all punctuation marks
    translator = str.maketrans('', '', string.punctuation)

    # Remove punctuation marks and standardize (remove spaces, convert to lowercase)
    llm_answer_clean = llm_answer.translate(translator).replace(" ", "").lower()
    correct_answer_clean = correct_answer.translate(translator).replace(" ", "").lower()

    return llm_answer_clean == correct_answer_clean


# generate the message for LLM to identify the accuracy of each sample
def message_generate(question: str, 
                     llm_answer: str, 
                     correct_answer: list
                     ) -> list[dict]:
    
    task_description = """You are an evaluator tasked with scoring the Candidate Answer.
Instructions:
1. Compare the Candidate Answer to the Correct Answer in the context of the Question.
2. Assign a score from 0 to 5 based on accuracy and completeness:
   - 0: Completely incorrect or irrelevant.
   - 1: Mostly incorrect with minimal relevance.
   - 2: Partially correct but contains significant errors.
   - 3: Moderately correct but lacks some details or precision.
   - 4: Mostly correct with minor inaccuracies.
   - 5: Fully correct and matches the Correct Answer.
3. Output ONLY the score as a single number (e.g. "3"). Do not include any explanations.
"""
    user_content = f"""Question: {question}
Correct Answer: {correct_answer}
Candidate Answer: {llm_answer}
"""
    messages = [
            {"role": "system", "content": task_description},
            {"role": "user", "content": user_content},
        ]
    return messages


# call api to get the accuracy score for a sample
# if error occures, return the error info else return the int type score
@sleep_and_retry
@limits(calls=RPM_LIMIT, period=60)
def Acc_evaluate_API(messages: list[dict]) -> int|str:
    base_url = MODEL_NAME_DICT["base_url"]
    model_id = MODEL_NAME_DICT["model_id"]
    api_key = MODEL_NAME_DICT["api_key"]

    client = OpenAI(base_url=base_url, api_key =api_key)

    # ask LLM API to get the response
    try:
        response = client.chat.completions.create(
            model=model_id,
            messages=messages,
            max_tokens=MAX_NEW_TOKENS,
        )
    except Exception as e:
        print(f"\nResponse Failed: {str(e)}\n")
        return f"Response Failed: {str(e)}"

    output_text = response.choices[0].message.content
    # match the 0-5 score in the output
    try:
        match = re.search(r'\d', output_text)
        match_score = int(match.group())
        if 0 <= match_score <= 5:
            return match_score
        else:
            print(f"\nResponse Invalid: {str(e)}\n")
            return f"Response Invalid: {output_text}"
    # match failed, return the output text directly
    except:
        print(f"\nResponse Parse Failed: {str(e)}\n")
        return f"Response Parse Failed: {output_text}"


# result write
# accuracy: list[float, int]
# float is the accuracy
# int is samples take part in the calculation of accuracy
def result_write(evaluator_write_path: str,
                 answer_reject_rate: float, 
                 answer_reject_dict: dict, 
                 accuracy: list[float, int]|None, 
                 accuracy_score_dict: dict
                 ) -> None:

    overall_evaluation = [{"Total Acc": accuracy, 
                          "Total Answer Reject Rate": answer_reject_rate}]
    each_sample_evaluation = [{"QID": QID,
                               "Answer Reject": answer_reject_dict[QID],
                               "Acc Score": accuracy_score_dict[QID]} 
                               for QID in answer_reject_dict]
    with open(evaluator_write_path, 'w', encoding='utf-8') as file:
        json.dump(overall_evaluation+each_sample_evaluation, file, ensure_ascii=False, indent=4)

    return


# evaluation function
def evaluation(llm_answers_path: str,
               correct_answers_path: str,
               evaluator_write_path: str
               ) -> None:

    # load correct_answers and questions
    with open(correct_answers_path, 'r', encoding='utf-8') as file:
        correct_answers = json.load(file)
    questions = {item['QID']: item["Question"]
                for item in correct_answers}
    correct_answers = {item['QID']: item["Answers"] 
                    for item in correct_answers}
    
    # load llm_answers
    with open(llm_answers_path, 'r', encoding='utf-8') as file:
        temp_llm_answers = json.load(file)
    temp_llm_answers = {item['QID']: item['Preds'] 
                   for item in temp_llm_answers}

    # keep llm_answers and correct_answers in the same length
    llm_answers = {}
    for QID in correct_answers:
        llm_answers[QID] = temp_llm_answers[QID]

    # calcuate answer_reject info and write
    answer_reject_rate, answer_reject_dict = answer_reject_calculate(llm_answers)
    print("answer reject info calculation completed")

    # initialize the accuracy and accuracy_score_dict
    accuracy = None
    accuracy_score_dict = {QID: None for QID in answer_reject_dict}

    # write result
    result_write(evaluator_write_path,
                 answer_reject_rate, 
                 answer_reject_dict, 
                 accuracy, 
                 accuracy_score_dict)

    # calcuate the accuracy_score for each sample
    total_samples = len(questions)
    pbar = tqdm(total=total_samples, desc="Acc Calculate")

    for QID in questions:
        # answer reject -- score=0
        if "I cannot answer" in llm_answers[QID]:
            accuracy_score_dict[QID] = 0

        # answer exact match -- score=5
        correct_answer_str = ''.join(correct_answers[QID])
        if answer_exact_match(llm_answers[QID], correct_answer_str):
            accuracy_score_dict[QID] = 5

        # use LLM to evaluate the acc score
        else:
            # debug: accuracy_score_dict[QID] = 3
            messages = message_generate(questions[QID], 
                                        llm_answers[QID], 
                                        correct_answers[QID])
            
            accuracy_score = Acc_evaluate_API(messages)
            accuracy_score_dict[QID] = accuracy_score
        
        pbar.update(1)

        if QID%30 == 0:
            result_write(evaluator_write_path,
                         answer_reject_rate, 
                         answer_reject_dict, 
                         accuracy, 
                         accuracy_score_dict)
    
    result_write(evaluator_write_path,
                 answer_reject_rate, 
                 answer_reject_dict, 
                 accuracy, 
                 accuracy_score_dict)

    # calculate the overall accuracy
    total_score = 0
    for _, score in accuracy_score_dict.items():
        if isinstance(score, int):
            total_score += score
        else:
            total_samples -= 1
    accuracy = [total_score/(total_samples*5), total_samples]
    print("\naccuracy info calculation completed")

    result_write(evaluator_write_path,
                 answer_reject_rate, 
                 answer_reject_dict, 
                 accuracy, 
                 accuracy_score_dict)
    print("result write completed")

    return


if __name__ == "__main__":
    llm_answers_paths_noise_00 = [
        "../CRAG-main/CRAG-main/data/nq/data_dir/main_n_00_10/output/pred_main_n_00_10_Llama-3.1-8B_old.json",
        "../CRAG-main/CRAG-main/data/nq/data_dir/main_n_00_10/output/pred_main_n_00_10_Llama-3.1-8B.json"
    ]
    
    llm_answers_paths_noise_01 = [
    "../CRAG-main/CRAG-main/data/nq/data_dir/main_n_01_10/output/pred_main_n_01_10_Llama-3.1-8B_old.json",
    "../CRAG-main/CRAG-main/data/nq/data_dir/main_n_01_10/output/pred_main_n_01_10_Llama-3.1-8B.json"
    ]

    correct_answers_paths = [
        "testbed/main_different_noise-ration/main_noise-ration-00.json",
        "testbed/main_different_noise-ration/main_noise-ration-01.json"
    ]

    correct_answers_path = correct_answers_paths[0]
    for i in range(2):
        llm_answers_path = llm_answers_paths_noise_00[i]
        evaluator_write_path = f"noise_00_{i}.json"
        evaluation(llm_answers_path, correct_answers_path, evaluator_write_path)

    correct_answers_path = correct_answers_paths[1]
    for i in range(2):
        llm_answers_path = llm_answers_paths_noise_01[i]
        evaluator_write_path = f"noise_01_{i}.json"
        evaluation(llm_answers_path, correct_answers_path, evaluator_write_path)
