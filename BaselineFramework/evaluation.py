# evaluate the experiment result based on the correct answer
import json
import string
import re
import os
import time
import asyncio
import argparse
from tqdm import tqdm
from typing import Tuple
from openai import OpenAI
from datetime import datetime, timedelta
from ratelimit import limits, sleep_and_retry
from concurrent.futures import ThreadPoolExecutor

# hyperparameter parse
parser = argparse.ArgumentParser()
# add hyperparameter
parser.add_argument("--experiment_name", type=str, required=True)
parser.add_argument("--variable_name", type=str, required=True)
parser.add_argument("--framework_name", type=str, required=True)
parser.add_argument("--model_name", type=str, required=True)
parser.add_argument("--variables", nargs="+", required=True)
parser.add_argument("--rpm_limit", type=int, required=True)
args = parser.parse_args()
# hyperparameter setting
experiment_name = args.experiment_name
variable_name = args.variable_name
framework_name = args.framework_name
model_name = args.model_name
variables = args.variables

RPM_LIMIT = args.rpm_limit
MAX_NEW_TOKENS = 20
# (付费版)阿里云百炼 https://bailian.console.aliyun.com/?spm=a2c4g.11186623.0.0.27ba516ecCpHkQ&accounttraceid=f54baffa43d5411a8c27405d70adde94qoxh#/model-market
MODEL_NAME_DICT = {"model_name": "Deepseek-v3",
                   "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
                   "model_id": "deepseek-v3",
                   "api_key": "sk-775b1e7c8a4d4a10b84508b0d6be1ba1"}

executor = ThreadPoolExecutor()



# calculate the answer reject rate
# and identify the answer reject of each sample
def answer_reject_calculate(llm_answers: dict) -> Tuple[float, dict]:
    total_samples = len(llm_answers)
    answer_reject_samples = 0
    answer_reject_dict = {}

    for QID, llm_answer in llm_answers.items():
        # "Response Failed:" Not participating in the evaluation
        if "Response Failed:" in llm_answer:
            total_samples -= 1
            answer_reject_dict[QID] = None
        # reject identify
        if "I cannot answer" in llm_answer:
            answer_reject_samples += 1
            answer_reject_dict[QID] = True
        else:
            answer_reject_dict[QID] = False

    answer_reject_rate = answer_reject_samples / total_samples
    return answer_reject_rate, answer_reject_dict


# answer exact match
# if matched, return true, else return false
def answer_exact_match(llm_answer: str, correct_answer: str) -> bool:
    """
    Compare the values corresponding to QID specified in two dictionaries,
    ignoring punctuation marks, spaces, and not case sensitive.

    : param llm_answers:  Dictionary containing answers generated by the model
    : param correct_answers:  A dictionary containing the correct answer
    : param QID:  Problem ID to be compared
    : return:  If the answer matches, return True; otherwise, return False
    """

    # Create a conversion table to remove all punctuation marks
    translator = str.maketrans('', '', string.punctuation)

    # Remove punctuation marks and standardize (remove spaces, convert to lowercase)
    llm_answer_clean = llm_answer.translate(translator).replace(" ", "").lower()
    correct_answer_clean = correct_answer.translate(translator).replace(" ", "").lower()

    return llm_answer_clean == correct_answer_clean


# generate the message for LLM to identify the accuracy of each sample
def message_generate(question: str,
                     llm_answer: str,
                     correct_answer: list
                     ) -> list[dict]:
    task_description = """You are an evaluator tasked with scoring the Candidate Answer.
Instructions:
1. Compare the Candidate Answer to the Correct Answer in the context of the Question.
2. Assign a score from 0 to 5 based on accuracy and completeness:
   - 0: Completely incorrect or irrelevant.
   - 1: Partially correct but contains significant errors.
   - 3: Moderately correct but lacks some details or precision.
   - 5: Fully correct and matches the Correct Answer.
3. Output ONLY the score as a single number (e.g. "3"). Do not include any explanations.
"""
    user_content = f"""Question: {question}
Correct Answer: {correct_answer}
Candidate Answer: {llm_answer}
"""
    messages = [
        {"role": "system", "content": task_description},
        {"role": "user", "content": user_content},
    ]
    return messages





# call api to get the accuracy score for a sample
# if error occures, return the error info else return the int type score
@sleep_and_retry
@limits(calls=RPM_LIMIT, period=60)
def Acc_evaluate_API(messages: list[dict]) -> int | str:
    base_url = MODEL_NAME_DICT["base_url"]
    model_id = MODEL_NAME_DICT["model_id"]
    api_key = MODEL_NAME_DICT["api_key"]

    client = OpenAI(base_url=base_url, api_key=api_key)

    # ask LLM API to get the response
    try:
        response = client.chat.completions.create(
            model=model_id,
            messages=messages,
            max_tokens=MAX_NEW_TOKENS,
        )
    except Exception as e:
        print(f"\n\nResponse Failed: {str(e)}")
        return f"Response Failed: {str(e)}"

    output_text = response.choices[0].message.content
    # match the 0-5 score in the output
    try:
        match = re.search(r'\d', output_text)
        match_score = int(match.group())
        if 0 <= match_score <= 5:
            return match_score
        else:
            print(f"\n\nResponse Failed: Invalid--{output_text}")
            return f"Response Failed: Invalid--{output_text}"
    # match failed, return the output text directly
    except Exception as e:
        print(f"\n\nResponse Failed: Parse Failed--{str(e)}")
        return f"Response Failed: Parse Failed--{output_text}"


@sleep_and_retry
@limits(calls=RPM_LIMIT, period=60)
async def async_inference(QID: int,
                          question: str,
                          correct_answer: list,
                          llm_answer: str):
    exclude_sample = False
    correct_answer_str = ', '.join(correct_answer)
    # "Response Failed:" -- Not participating in the evaluation
    if "Response Failed:" in llm_answer:
        accuracy_score = None
        exclude_sample = True

    # answer reject -- score=0
    elif "I cannot answer" in llm_answer:
        accuracy_score = 0

    # answer exact match -- score=5
    elif answer_exact_match(llm_answer, correct_answer_str):
        accuracy_score = 5

    # use LLM to evaluate the acc score
    else:
        messages = message_generate(question,
                                    llm_answer,
                                    correct_answer)

        loop = asyncio.get_event_loop()
        accuracy_score = await loop.run_in_executor(executor, 
                                                    Acc_evaluate_API, 
                                                    messages)
    return QID, accuracy_score, exclude_sample


# result write
# accuracy: list[float, int]
# float is the accuracy
# int is samples take part in the calculation of accuracy
def result_write(evaluator_write_path: str,
                 answer_reject_rate: float,
                 answer_reject_dict: dict,
                 accuracy: list[float, int] | None,
                 accuracy_score_dict: dict
                 ) -> None:
    overall_evaluation = [{"Total Acc": accuracy,
                           "Total Answer Reject Rate": answer_reject_rate}]
    each_sample_evaluation = [{"QID": QID,
                               "Answer Reject": answer_reject_dict[QID],
                               "Acc Score": accuracy_score_dict[QID]}
                              for QID in answer_reject_dict]
    with open(evaluator_write_path, 'w', encoding='utf-8') as file:
        json.dump(overall_evaluation + each_sample_evaluation, file, ensure_ascii=False, indent=4)

    return


# evaluation function
# The samples used for evaluation are samples from the testbed (correct_answers_path)
# but those that failed to generate by frmaework are excluded
# "failed to generate" refers to recording in llm_answers_path: "Response Failed:"
async def evaluation(framework: str,
               llm_answers_path: str,
               correct_answers_path: str,
               evaluator_write_path: str
               ) -> None:
    # load correct_answers and questions
    with open(correct_answers_path, 'r', encoding='utf-8') as file:
        correct_answers = json.load(file)
    questions = {item['QID']: item["Question"]
                 for item in correct_answers}
    correct_answers = {item['QID']: item["Answers"]
                       for item in correct_answers}

    # load llm_answers
    with open(llm_answers_path, 'r', encoding='utf-8') as file:
        temp_llm_answers = json.load(file)
    temp_llm_answers = {item['QID']: item['Output Answer']
                        for item in temp_llm_answers}

    # keep llm_answers and correct_answers in the same length
    llm_answers = {}
    for QID in correct_answers:
        llm_answers[QID] = temp_llm_answers[QID]

    # parse the Reading Note and Answer for CoN framework
    if framework == "ChainofNote":
        for QID in llm_answers:
            answer_match = re.search(r"Answer:\s*(.*)",
                                     llm_answers[QID],
                                     re.DOTALL)
        if answer_match:
            llm_answers[QID] = answer_match.group(1).strip()

    # calcuate answer_reject info and write
    answer_reject_rate, answer_reject_dict = answer_reject_calculate(llm_answers)
    print("answer reject info calculation completed")

    # initialize the accuracy and accuracy_score_dict
    accuracy = None
    accuracy_score_dict = {QID: None for QID in answer_reject_dict}

    # write result
    result_write(evaluator_write_path,
                 answer_reject_rate,
                 answer_reject_dict,
                 accuracy,
                 accuracy_score_dict)

    # calcuate the accuracy_score for each sample
    total_samples = len(questions)
    pbar = tqdm(total=total_samples, desc="Acc Calculate")

    tasks = []

    batch_size = 400
    questions_list = list(questions.items())
    for i in range(0, len(questions_list), batch_size):
        questions_batch = dict(questions_list[i:i + batch_size])

        for QID in questions_batch:
            # creating an asynchronous task
            task = asyncio.create_task(async_inference(QID,
                                                       questions[QID],
                                                       correct_answers[QID],
                                                       llm_answers[QID]))
            tasks.append(task)

        # LLM generation
        results_of_tasks = await asyncio.gather(*tasks)

        for QID, accuracy_score, exclude_sample in results_of_tasks:
            accuracy_score_dict[QID] = accuracy_score
            if exclude_sample:
                total_samples -= 1

        result_write(evaluator_write_path,
                     answer_reject_rate,
                     answer_reject_dict,
                     accuracy,
                     accuracy_score_dict)
        
        pbar.update(batch_size)

    # calculate the overall accuracy
    total_score = 0
    for _, score in accuracy_score_dict.items():
        if isinstance(score, int):
            total_score += score
        else:
            total_samples -= 1
    accuracy = [total_score / (total_samples * 5), total_samples]
    print("\naccuracy info calculation completed")

    result_write(evaluator_write_path,
                 answer_reject_rate,
                 answer_reject_dict,
                 accuracy,
                 accuracy_score_dict)
    print("result write completed")

    return


if __name__ == "__main__":

    print("------Evaluation Total Setting------")
    print("Experiment Name", experiment_name)
    print("Variable Name", variable_name)
    print("Variable List", variables)
    print("Framework Name: ", framework_name)
    print("Model Name: ", model_name)
    print("------------------------------------\n")

    for variable in variables:
        # set hyperparameters
        parse_model_name = model_name.replace(".", "-")
        llm_answers_path = f"ExperimentResult/model_output/{experiment_name}_different_{variable_name}/{framework_name}/{experiment_name}_{variable_name}-{variable}_{framework_name}_{parse_model_name}.json"
        correct_answers_path = f"testbed/{experiment_name}_different_{variable_name}/{experiment_name}_{variable_name}-{variable}.json"
        evaluator_write_path = f"ExperimentResult/evaluation_result/{experiment_name}_different_{variable_name}/{framework_name}/{experiment_name}_{variable_name}-{variable}_{framework_name}_{parse_model_name}.json"

        pid = os.getpid()

        now = datetime.now()
        formatted_start_time = now.strftime("%Y-%m-%d %H:%M:%S")
        start_time = time.time()

        print("\n")
        print("------Evaluation Start------")
        print("Settings")
        print("PID", pid)

        print("LLM Answer Path", llm_answers_path)
        print("Correct Answer Path", correct_answers_path)
        print("Evaluation Write Path", evaluator_write_path)
        print("Start Time: ", formatted_start_time)

        # start evaluation
        asyncio.run(evaluation(framework_name,
                               llm_answers_path,
                               correct_answers_path,
                               evaluator_write_path))

        end_time = time.time()
        now = datetime.now()
        formatted_end_time = now.strftime("%Y-%m-%d %H:%M:%S")

        delta = timedelta(seconds=end_time - start_time)
        print("\n")
        print("End Time: ", formatted_end_time)
        print("Time Cost: ", delta)
        print("\n")